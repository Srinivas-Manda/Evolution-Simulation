{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.10.9)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from Environment.custom_env import CustomEnvironment\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "from Models.ActorCritic import ActorCritic\n",
    "from Models.SoftActorCritic import SoftActorCritic\n",
    "from Models.DDQN import DoubleDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function TextIOWrapper.close()>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('Environment/env_config.json')\n",
    "env_variables = json.load(f)\n",
    "f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CustomEnvironment(env_config=env_variables, render_mode=\"human\")\n",
    "# env = parallel_env(render_mode=\"human\")\n",
    "# env.reset(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mms = MinMaxScaler(feature_range=(0, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function TextIOWrapper.close()>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ac_config = open('Environment/ac_config.json')\n",
    "ac_variables = json.load(ac_config)\n",
    "ac_config.close\n",
    "\n",
    "sac_config = open('Environment/sac_config.json')\n",
    "sac_variables = json.load(sac_config)\n",
    "sac_config.close\n",
    "\n",
    "ddqn_config = open('Environment/ddqn_config.json')\n",
    "ddqn_variables = json.load(ddqn_config)\n",
    "ddqn_config.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddqn_1 = DoubleDQN(ddqn_variables)\n",
    "# ddqn_2 = DoubleDQN(ddqn_variables)\n",
    "# ddqn_3 = DoubleDQN(ddqn_variables)\n",
    "# ac = ActorCritic(ac_variables)\n",
    "# sac = SoftActorCritic(sac_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {0:ddqn_1, 1:ddqn_1, 2:ddqn_1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grad_flow(named_parameters):\n",
    "    '''Plots the gradients flowing through different layers in the net during training.\n",
    "    Can be used for checking for possible gradient vanishing / exploding problems.\n",
    "    \n",
    "    Usage: Plug this function in Trainer class after loss.backwards() as \n",
    "    \"plot_grad_flow(self.model.named_parameters())\" to visualize the gradient flow'''\n",
    "    ave_grads = []\n",
    "    max_grads= []\n",
    "    layers = []\n",
    "    for i, p in enumerate(named_parameters):\n",
    "        \n",
    "        if(p.requires_grad):\n",
    "            layers.append(i)\n",
    "            ave_grads.append(p.grad.cpu().abs().mean())\n",
    "            max_grads.append(p.grad.cpu().abs().max())\n",
    "\n",
    "    plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.1, lw=1, color=\"c\")\n",
    "    plt.bar(np.arange(len(max_grads)), ave_grads, alpha=0.1, lw=1, color=\"b\")\n",
    "    plt.hlines(0, 0, len(ave_grads)+1, lw=2, color=\"k\" )\n",
    "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
    "    plt.xlim(left=0, right=len(ave_grads))\n",
    "    plt.ylim(bottom = -0.001, top=0.02) # zoom in on the lower gradient regions\n",
    "    plt.xlabel(\"Layers\")\n",
    "    plt.ylabel(\"average gradient\")\n",
    "    plt.title(\"Gradient flow\")\n",
    "    plt.grid(True)\n",
    "    plt.legend([Line2D([0], [0], color=\"c\", lw=4),\n",
    "                Line2D([0], [0], color=\"b\", lw=4),\n",
    "                Line2D([0], [0], color=\"k\", lw=4)], ['max-gradient', 'mean-gradient', 'zero-gradient'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b845d44de4374e16b4bc3e3eb63d95b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.57361960959597, 1: 0.41840087908508217, 2: 0.5500019841413828}\n",
      "agent1 died at time 99\n",
      "agent2 died at time 99\n",
      "{0: 0.8336116135629557, 1: 0.6759987512175292, 2: 0.49534470079100246}\n",
      "agent0 died at time 120\n",
      "Episode 0 end\n",
      "321\n",
      "Losses {1: tensor(92.2216), 2: tensor(23.9393), 0: tensor(125.9719)}\n",
      "\n",
      "{0: 0.7916823780303901, 1: 0.8077581641352923, 2: 0.6154808045809317}\n",
      "agent0 died at time 99\n",
      "{0: 0.6157288555237312, 1: -3, 2: 0.7563145934866299}\n",
      "agent1 died at time 141\n",
      "{2: 0.783925396360568}\n",
      "{2: -3}\n",
      "{2: -3}\n",
      "agent2 died at time 267\n",
      "Episode 1 end\n",
      "831\n",
      "Losses {0: tensor(19.2773), 1: tensor(14.3475), 2: tensor(30.0516)}\n",
      "\n",
      "{0: 0.8399272996240852, 1: 0.6087921146182202, 2: 0.6016274766090621}\n",
      "{0: 0.42334157657691474, 1: 0.6543582860635688, 2: 0.6488736037190399}\n",
      "agent0 died at time 120\n",
      "agent1 died at time 120\n",
      "agent2 died at time 120\n",
      "Episode 2 end\n",
      "1194\n",
      "Losses {0: tensor(218.6519), 1: tensor(149.0945), 2: tensor(24.4748)}\n",
      "\n",
      "{0: 0.42935673448467426, 1: 0.6106648398075893, 2: 0.7181508266215624}\n",
      "agent2 died at time 99\n",
      "{0: 0.43793714961259067, 1: 0.4134158461808547, 2: 0.846154754714836}\n",
      "agent0 died at time 141\n",
      "{1: 0.6313003465875927}\n",
      "agent1 died at time 162\n",
      "Episode 3 end\n",
      "1599\n",
      "Losses {2: tensor(212.4905), 0: tensor(320.3591), 1: tensor(133.1896)}\n",
      "\n",
      "{0: -3, 1: 55, 2: 0.3925682948267324}\n",
      "{0: 0.7125810545244464, 1: 0.8114334865557693, 2: 55}\n",
      "agent2 died at time 141\n",
      "{0: 0.31471725019095464, 1: 0.6260975054515604}\n",
      "{0: 0.2863434886408859, 1: 0.6233468007462835}\n",
      "agent1 died at time 204\n",
      "agent0 died at time 225\n",
      "Episode 4 end\n",
      "2172\n",
      "Losses {2: tensor(168.9358), 1: tensor(264.1754), 0: tensor(168.8179)}\n",
      "\n",
      "{0: 0.6352231812718296, 1: 0.5299510276730817, 2: 0.6084227321485005}\n",
      "{0: 0.706122070017625, 1: 0.7993168535342456, 2: 0.0}\n",
      "agent2 died at time 120\n",
      "agent1 died at time 141\n",
      "{0: 0.7403085223960775}\n",
      "{0: 0.14818508416150944}\n",
      "{0: 0.33748200531477623}\n",
      "agent0 died at time 267\n",
      "Episode 5 end\n",
      "2703\n",
      "Losses {2: tensor(151.7741), 1: tensor(128.1370), 0: tensor(286.4330)}\n",
      "\n",
      "{0: -3, 1: 0.7224533311627197, 2: 0.6164835834705984}\n",
      "{0: -3, 1: 0.6552647896110273, 2: 0.7868412986905625}\n",
      "agent0 died at time 120\n",
      "agent2 died at time 120\n",
      "{1: 0.5451814357836584}\n",
      "agent1 died at time 183\n",
      "Episode 6 end\n",
      "3129\n",
      "Losses {0: tensor(122.9429), 2: tensor(774.3448), 1: tensor(159.2210)}\n",
      "\n",
      "{0: 0.6288492375585479, 1: 0.6997211850595502, 2: 0.5064395628642381}\n",
      "{0: 0.7223534781865353, 1: 0.8674048633058296, 2: 0.9008170763345525}\n",
      "agent2 died at time 141\n",
      "{0: 0.341295866194828, 1: 0.8956781322124931}\n",
      "agent0 died at time 162\n",
      "agent1 died at time 183\n",
      "Episode 7 end\n",
      "3618\n",
      "Losses {2: tensor(284.5328), 0: tensor(173.7604), 1: tensor(176.0001)}\n",
      "\n",
      "{0: 0.7964141437287046, 1: 0.5580875938421386, 2: -3}\n",
      "{0: 0.5221744680871331, 1: 0.731487233350487, 2: 0.34160362691555235}\n",
      "agent2 died at time 141\n",
      "{0: 0.5453434977053275, 1: 0.17207204340312854}\n",
      "agent0 died at time 162\n",
      "agent1 died at time 162\n",
      "Episode 8 end\n",
      "4086\n",
      "Losses {2: tensor(125.7218), 0: tensor(187.0486), 1: tensor(1876.9850)}\n",
      "\n",
      "{0: 0.48463787909893474, 1: 0.0, 2: -3}\n",
      "{0: -3, 1: 0.21819187693798392, 2: 0.0}\n",
      "agent0 died at time 120\n",
      "agent1 died at time 120\n",
      "agent2 died at time 141\n",
      "Episode 9 end\n",
      "4470\n",
      "Losses {0: tensor(181.2854), 1: tensor(99.8552), 2: tensor(217.5095)}\n",
      "\n",
      "{0: 0.541384687516907, 1: 0.6534976374769771, 2: 0.44875480375853727}\n",
      "agent1 died at time 99\n",
      "{0: 0.4467096076856273, 1: 0.860003932537287, 2: 55}\n",
      "agent0 died at time 120\n",
      "agent2 died at time 141\n",
      "Episode 10 end\n",
      "4833\n",
      "Losses {1: tensor(207.5765), 0: tensor(139.1506), 2: tensor(428.4632)}\n",
      "\n",
      "{0: 0.6537038400843629, 1: 0.27774710640832045, 2: 0.7571510104147817}\n",
      "{0: 0.6026423023085772, 1: 0.4902056366646035, 2: 0.3397574264452743}\n",
      "agent2 died at time 141\n",
      "{0: 0.5514368530827294, 1: 0.6759444754084963}\n",
      "agent1 died at time 162\n",
      "{0: 0.35515372737766204}\n",
      "{0: -3}\n",
      "agent0 died at time 288\n",
      "Episode 11 end\n",
      "5427\n",
      "Losses {2: tensor(139.8487), 1: tensor(194.5043), 0: tensor(194.3129)}\n",
      "\n",
      "{0: 0.7198884297958328, 1: 0.6935909009510979, 2: 0.8447103150052342}\n",
      "{0: 0.7587064242364849, 1: 0.7639980870982165, 2: 0.5565825012048458}\n",
      "{0: -3, 1: 0.3234769532742937, 2: 0.4855899403064998}\n",
      "agent0 died at time 162\n",
      "agent2 died at time 183\n",
      "{1: 0.7913819459676682}\n",
      "agent1 died at time 204\n",
      "Episode 12 end\n",
      "5979\n",
      "Losses {0: tensor(107.8144), 2: tensor(101.2447), 1: tensor(232.4513)}\n",
      "\n",
      "{0: 0.7513864086430823, 1: 55, 2: 0.7357647852509426}\n",
      "{0: -3, 1: 0.5447371745627065, 2: 0.4489693822544898}\n",
      "{0: 0.0, 1: 0.3109046134857093, 2: 0.41570327346434255}\n",
      "agent0 died at time 162\n",
      "agent2 died at time 162\n",
      "{1: 0.8308061511895988}\n",
      "agent1 died at time 204\n",
      "Episode 13 end\n",
      "6510\n",
      "Losses {0: tensor(158.8560), 2: tensor(53.9452), 1: tensor(108.3327)}\n",
      "\n",
      "{0: 0.5129396041853092, 1: 0.6677425692843699, 2: 0.6891465503911822}\n",
      "{0: 0.3826399409443805, 1: 0.0, 2: 0.5948957028350117}\n",
      "agent1 died at time 141\n",
      "{0: 0.6747014186808666, 2: 0.9177536605268115}\n",
      "agent2 died at time 183\n",
      "{0: 0.7276639453105496}\n",
      "agent0 died at time 204\n",
      "Episode 14 end\n",
      "7041\n",
      "Losses {1: tensor(104.4605), 2: tensor(106.4413), 0: tensor(35.8487)}\n",
      "\n",
      "{0: 0.32175228237513454, 1: 0.46690021580848295, 2: 0.7121929637799593}\n",
      "{0: 0.7000659369658331, 1: 0.35544097777192607, 2: 0.5000416290846663}\n",
      "agent1 died at time 120\n",
      "{0: 0.647805911898648, 2: 0.6264700166878072}\n",
      "agent2 died at time 162\n",
      "{0: 0.557026645076425}\n",
      "agent0 died at time 225\n",
      "Episode 15 end\n",
      "7551\n",
      "Losses {1: tensor(49.1006), 2: tensor(42.4547), 0: tensor(51.5425)}\n",
      "\n",
      "{0: 0.4650881752657494, 1: 0.32334646258966393, 2: 0.32686322273936186}\n",
      "{0: 0.85275200815082, 1: 0.40800925470022487, 2: 0.5702050238106615}\n",
      "agent1 died at time 141\n",
      "{0: 0.562756028845431, 2: -3}\n",
      "agent2 died at time 162\n",
      "{0: 0.3530534167357702}\n",
      "agent0 died at time 204\n",
      "Episode 16 end\n",
      "8061\n",
      "Losses {1: tensor(193.7863), 2: tensor(470.9218), 0: tensor(47.9445)}\n",
      "\n",
      "{0: 0.3349144992686145, 1: 0.5457939712950617, 2: 0.7708612011794705}\n",
      "{0: 0.6617522210081075, 1: 0.8329939938125401, 2: 0.535473587096827}\n",
      "agent1 died at time 120\n",
      "agent2 died at time 120\n",
      "agent0 died at time 141\n",
      "Episode 17 end\n",
      "8445\n",
      "Losses {1: tensor(145.5929), 2: tensor(160.2997), 0: tensor(36.6237)}\n",
      "\n",
      "{0: 0.7286069008310392, 1: 0.6946841808065312, 2: 0.6453382541969422}\n",
      "{0: 0.775463990110396, 1: 0.2884516698411028, 2: 0.5244654917218146}\n",
      "agent1 died at time 120\n",
      "{0: 0.6900416405690089, 2: 0.6525237421027157}\n",
      "agent0 died at time 183\n",
      "agent2 died at time 183\n",
      "Episode 18 end\n",
      "8934\n",
      "Losses {1: tensor(169.3120), 0: tensor(129.6779), 2: tensor(189.9771)}\n",
      "\n",
      "{0: 0.5755697245916366, 1: 0.7987957817848795, 2: 0.6439541030107936}\n",
      "agent0 died at time 99\n",
      "{0: 0.7859656911722745, 1: 0.6934529173295267, 2: 0.6331816651315865}\n",
      "{1: 0.3250065897683484, 2: 0.7315408265934027}\n",
      "agent1 died at time 162\n",
      "agent2 died at time 162\n",
      "Episode 19 end\n",
      "9360\n",
      "Losses {0: tensor(25.8168), 1: tensor(251.4712), 2: tensor(81.5000)}\n",
      "\n",
      "{0: 0.5317381946633489, 1: 0.8509644957454653, 2: 0.535425778511154}\n",
      "{0: 0.4583070030149672, 1: -3, 2: 0.8036809833528212}\n",
      "agent1 died at time 141\n",
      "{0: -3, 2: 0.6500358401694111}\n",
      "agent0 died at time 162\n",
      "agent2 died at time 162\n",
      "Episode 20 end\n",
      "9828\n",
      "Losses {1: tensor(103.5286), 0: tensor(153.9361), 2: tensor(64.6909)}\n",
      "\n",
      "{0: 0.5232085788259531, 1: 0.8311744640939935, 2: 0.5896825779241409}\n",
      "{0: 0.8428210719615526, 1: 0.7459337947822474, 2: 0.6357035546248392}\n",
      "agent1 died at time 141\n",
      "agent2 died at time 141\n",
      "{0: 0.568330740389992}\n",
      "{0: -3}\n",
      "agent0 died at time 204\n",
      "Episode 21 end\n",
      "10317\n",
      "Losses {1: tensor(118.1551), 2: tensor(149.0056), 0: tensor(13.8253)}\n",
      "\n",
      "{0: 0.7208927325655641, 1: 0.6742427980572918, 2: 0.7712985912316558}\n",
      "agent1 died at time 99\n",
      "{0: 0.6109035805589064, 1: 0.7088623788131344, 2: 0.7252013833267805}\n",
      "{0: 0.35241214097750384, 2: 0.45480106822538113}\n",
      "agent0 died at time 162\n",
      "agent2 died at time 162\n",
      "Episode 22 end\n",
      "10743\n",
      "Losses {1: tensor(37.1842), 0: tensor(140.8715), 2: tensor(78.7846)}\n",
      "\n",
      "{0: 0.4135287195207469, 1: 0.38670422303469576, 2: 0.7003930754244425}\n",
      "{0: 0.669241263648896, 1: 0.4649101833580215, 2: 0.5348641624919948}\n",
      "agent1 died at time 120\n",
      "{0: 0.5688044478590581, 2: 0.6644017398635146}\n",
      "agent0 died at time 162\n",
      "agent2 died at time 162\n",
      "Episode 23 end\n",
      "11190\n",
      "Losses {1: tensor(54.0688), 0: tensor(608.7924), 2: tensor(213.1789)}\n",
      "\n",
      "{0: 0.6175017535281899, 1: 0.8080856004988768, 2: 0.416136779154612}\n",
      "{0: -3, 1: 0.38360415697217687, 2: 55}\n",
      "agent0 died at time 141\n",
      "{1: 0.6576794881675718, 2: 0.45734987181900233}\n",
      "agent1 died at time 162\n",
      "agent2 died at time 162\n",
      "Episode 24 end\n",
      "11658\n",
      "Losses {0: tensor(452.8958), 1: tensor(159.7495), 2: tensor(146.5254)}\n",
      "\n",
      "{0: 0.7755984494192858, 1: 0.7195578891611467, 2: 0.7910321524323145}\n",
      "{0: 0.6922893050132889, 1: 0.35563819658740603, 2: 0.6403794266690226}\n",
      "agent1 died at time 141\n",
      "{0: 0.6463006827619848, 2: 0.7191127793658878}\n",
      "agent0 died at time 162\n",
      "{2: 0.7369469639154482}\n",
      "{2: 0.7064105413689561}\n",
      "agent2 died at time 288\n",
      "Episode 25 end\n",
      "12252\n",
      "Losses {1: tensor(13.1076), 0: tensor(65.9127), 2: tensor(155.3814)}\n",
      "\n",
      "{0: 0.8512475159349336, 1: 0.5630144340396375, 2: -3}\n",
      "agent2 died at time 99\n",
      "{0: 0.6359873214442434, 1: 0.4005978207732297, 2: -3}\n",
      "agent1 died at time 120\n",
      "{0: 0.1952054760461066}\n",
      "agent0 died at time 183\n",
      "Episode 26 end\n",
      "12657\n",
      "Losses {2: tensor(64.3555), 1: tensor(155.6341), 0: tensor(93.5630)}\n",
      "\n",
      "{0: 0.47965474673221487, 1: 0.7375981767190477, 2: -3}\n",
      "agent2 died at time 99\n",
      "{0: 0.4899658449165749, 1: 0.521588714666676, 2: -3}\n",
      "agent0 died at time 120\n",
      "{1: 0.0}\n",
      "{1: -3}\n",
      "agent1 died at time 246\n",
      "Episode 27 end\n",
      "13125\n",
      "Losses {2: tensor(115.2637), 0: tensor(355.4709), 1: tensor(188.6216)}\n",
      "\n",
      "{0: 0.6733096538486345, 1: 0.6672920100992141, 2: 0.6242466343357231}\n",
      "agent1 died at time 99\n",
      "{0: 0.5064385829631324, 1: -3, 2: 0.7619399626280193}\n",
      "agent0 died at time 141\n",
      "{2: -3}\n",
      "agent2 died at time 162\n",
      "Episode 28 end\n",
      "13530\n",
      "Losses {1: tensor(174.5383), 0: tensor(293.2381), 2: tensor(141.6997)}\n",
      "\n",
      "{0: 0.5291059098188844, 1: -3, 2: 0.47204236277612444}\n",
      "{0: 0.7689158632723516, 1: 0.7529540168744052, 2: 0.6366330208870459}\n",
      "agent1 died at time 120\n",
      "agent0 died at time 141\n",
      "{2: 0.5075405778911968}\n",
      "agent2 died at time 183\n",
      "Episode 29 end\n",
      "13977\n",
      "Losses {1: tensor(36.2819), 0: tensor(29.7052), 2: tensor(112.3324)}\n",
      "\n",
      "{0: 0.8003135143825743, 1: 0.768571394579695, 2: 0.542155981729946}\n",
      "agent0 died at time 99\n",
      "{0: 0.5436002526564139, 1: 0.6462121918856083, 2: 0.6215928307646421}\n",
      "agent1 died at time 120\n",
      "{2: 0.6575178937834258}\n",
      "{2: 0.7959623170549157}\n",
      "agent2 died at time 246\n",
      "Episode 30 end\n",
      "14445\n",
      "Losses {0: tensor(48.0448), 1: tensor(96.6595), 2: tensor(18.6791)}\n",
      "\n",
      "{0: 0.766887225138201, 1: 0.5782232541653782, 2: 0.7334745792036086}\n",
      "{0: 0.5309148742301681, 1: 0.5646818955598927, 2: 0.7427387031701436}\n",
      "agent2 died at time 141\n",
      "{0: 0.7185492793593802, 1: -3}\n",
      "agent1 died at time 162\n",
      "{0: -3}\n",
      "agent0 died at time 225\n",
      "Episode 31 end\n",
      "14976\n",
      "Losses {2: tensor(18.3549), 1: tensor(20.0921), 0: tensor(29.1755)}\n",
      "\n",
      "{0: 0.45384990719591156, 1: 0.8773166554840878, 2: 0.5336064804485527}\n",
      "{0: 0.6260951165330071, 1: 0.6972155384090652, 2: 0.7815462417622241}\n",
      "agent0 died at time 120\n",
      "{1: -3, 2: 0.6132383350172266}\n",
      "agent2 died at time 162\n",
      "{1: -3}\n",
      "agent1 died at time 204\n",
      "Episode 32 end\n",
      "15465\n",
      "Losses {0: tensor(114.1367), 2: tensor(156.7564), 1: tensor(17.1449)}\n",
      "\n",
      "{0: 0.8252049279643189, 1: 0.616736512707745, 2: 0.6850832721553017}\n",
      "{0: 0.6028143377309807, 1: 0.39919904866597156, 2: 0.6686267003813318}\n",
      "agent0 died at time 120\n",
      "agent2 died at time 120\n",
      "agent1 died at time 141\n",
      "Episode 33 end\n",
      "15849\n",
      "Losses {0: tensor(11.4289), 2: tensor(94.2755), 1: tensor(12.9590)}\n",
      "\n",
      "{0: 0.44293025962063914, 1: 0.3482840146088516, 2: 0.6302588023514661}\n",
      "agent0 died at time 99\n",
      "{0: 0.7103496456970007, 1: 0.672300547785875, 2: 0.4500360070874966}\n",
      "agent1 died at time 141\n",
      "{2: -3}\n",
      "agent2 died at time 162\n",
      "Episode 34 end\n",
      "16254\n",
      "Losses {0: tensor(115.4619), 1: tensor(122.5513), 2: tensor(20.0603)}\n",
      "\n",
      "{0: 0.8245371703864472, 1: 0.867024197716663, 2: 0.6677511007677936}\n",
      "agent2 died at time 99\n",
      "{0: 0.7477861451990422, 1: 0.6923729060015055, 2: 0.41244375368736974}\n",
      "{0: 0.5766410859116734, 1: 0.0}\n",
      "agent1 died at time 183\n",
      "{0: 0.6786847000603462}\n",
      "{0: -3}\n",
      "agent0 died at time 288\n",
      "Episode 35 end\n",
      "16827\n",
      "Losses {2: tensor(136.5715), 1: tensor(13.4367), 0: tensor(115.2843)}\n",
      "\n",
      "{0: 0.6333585698317505, 1: 0.38331195301776666, 2: 0.8478212599431736}\n",
      "{0: 0.21999735262023856, 1: 0.46045669380590004, 2: 0.5698619331529766}\n",
      "agent0 died at time 141\n",
      "{1: 0.0, 2: 0.4429571328397791}\n",
      "agent1 died at time 162\n",
      "agent2 died at time 162\n",
      "Episode 36 end\n",
      "17295\n",
      "Losses {0: tensor(11.6192), 1: tensor(4.7026), 2: tensor(279.3593)}\n",
      "\n",
      "{0: 0.7899014449780782, 1: 0.5352585858974819, 2: 0.7470893607158005}\n",
      "{0: 0.37277874006579803, 1: 0.5350564398828399, 2: 0.743247133656807}\n",
      "agent1 died at time 120\n",
      "agent2 died at time 141\n",
      "{0: 0.6808428760093521}\n",
      "agent0 died at time 162\n",
      "Episode 37 end\n",
      "17721\n",
      "Losses {1: tensor(20.1055), 2: tensor(56.8536), 0: tensor(79.1327)}\n",
      "\n",
      "{0: 0.6885496987667763, 1: 0.4852686600543793, 2: 0.7781603745754181}\n",
      "{0: 0.5403050040016081, 1: 0.6340832951721529, 2: 55}\n",
      "agent0 died at time 120\n",
      "agent1 died at time 120\n",
      "{2: 0.49402067782401504}\n",
      "agent2 died at time 162\n",
      "Episode 38 end\n",
      "18126\n",
      "Losses {0: tensor(126.6647), 1: tensor(18.8866), 2: tensor(173.3709)}\n",
      "\n",
      "{0: 0.31908572220727793, 1: 0.9483927994595952, 2: 0.752292030181337}\n",
      "{0: 0.7530883320012943, 1: 0.49049180161263817, 2: 0.6656499699341796}\n",
      "agent0 died at time 120\n",
      "agent1 died at time 141\n",
      "{2: 0.5894074645067531}\n",
      "agent2 died at time 162\n",
      "Episode 39 end\n",
      "18552\n",
      "Losses {0: tensor(113.4860), 1: tensor(30.3077), 2: tensor(185.6695)}\n",
      "\n",
      "{0: 0.30689636508666973, 1: 0.8332535416171183, 2: 0.6805624749316074}\n",
      "{0: 0.5410842666693199, 1: 0.4822308497352409, 2: 0.4415500177579218}\n",
      "agent0 died at time 141\n",
      "agent1 died at time 141\n",
      "{2: 0.31710379169937997}\n",
      "agent2 died at time 162\n",
      "Episode 40 end\n",
      "18999\n",
      "Losses {0: tensor(138.2437), 1: tensor(247.4049), 2: tensor(34.5774)}\n",
      "\n",
      "{0: 0.39717143501479224, 1: 0.43801064657635347, 2: 0.5820789058097233}\n",
      "{0: 0.4252776391790921, 1: 0.43033091740611606, 2: -3}\n",
      "agent0 died at time 120\n",
      "agent1 died at time 120\n",
      "{2: -3}\n",
      "agent2 died at time 162\n",
      "Episode 41 end\n",
      "19404\n",
      "Losses {0: tensor(47.5110), 1: tensor(83.2406), 2: tensor(32.4057)}\n",
      "\n",
      "{0: 0.38045603871650135, 1: 0.4072485815137815, 2: 0.6135450681279181}\n",
      "{0: 0.4642102719564838, 1: 0.711885753083031, 2: 0.5964792750756283}\n",
      "agent0 died at time 141\n",
      "{1: 0.7827179876348417, 2: 0.5207763672727993}\n",
      "agent2 died at time 162\n",
      "{1: 0.8842054173197638}\n",
      "agent1 died at time 204\n",
      "Episode 42 end\n",
      "19914\n",
      "Losses {0: tensor(150.6611), 2: tensor(190.1667), 1: tensor(24.5339)}\n",
      "\n",
      "{0: 0.703261101102159, 1: 0.5706651945773284, 2: 0.5832805855622929}\n",
      "agent0 died at time 99\n",
      "{0: 0.6170539320332329, 1: 0.5509231791014956, 2: 0.681101140959331}\n",
      "agent1 died at time 120\n",
      "{2: 0.6096800141085912}\n",
      "agent2 died at time 183\n",
      "Episode 43 end\n",
      "20319\n",
      "Losses {0: tensor(18.5540), 1: tensor(33.9538), 2: tensor(110.3712)}\n",
      "\n",
      "{0: 0.8892440585100513, 1: 0.7264867568875377, 2: 0.8143465483643808}\n",
      "{0: 0.3819963823397252, 1: 0.9412306588201155, 2: 0.5670976846328257}\n",
      "agent2 died at time 141\n",
      "{0: -3, 1: 0.5691955253479797}\n",
      "agent0 died at time 162\n",
      "{1: -3}\n",
      "agent1 died at time 204\n",
      "Episode 44 end\n",
      "20829\n",
      "Losses {2: tensor(68.5765), 0: tensor(179.7086), 1: tensor(114.6915)}\n",
      "\n",
      "{0: 55, 1: 0.661112505389431, 2: 0.4899856571406369}\n",
      "{0: 0.8339544294217309, 1: 0.6094170301723911, 2: -3}\n",
      "agent1 died at time 141\n",
      "agent2 died at time 141\n",
      "{0: 0.773441411293879}\n",
      "agent0 died at time 183\n",
      "Episode 45 end\n",
      "21297\n",
      "Losses {1: tensor(20.0554), 2: tensor(21.4815), 0: tensor(16.7372)}\n",
      "\n",
      "{0: 0.7622768177308484, 1: 0.7895931978395156, 2: 0.6263410771319782}\n",
      "{0: 0.7779207898725181, 1: 0.7145840047841729, 2: 0.7958056218196761}\n",
      "agent1 died at time 120\n",
      "agent2 died at time 141\n",
      "{0: 0.6304641859107466}\n",
      "agent0 died at time 183\n",
      "Episode 46 end\n",
      "21744\n",
      "Losses {1: tensor(136.5282), 2: tensor(134.9991), 0: tensor(30.2146)}\n",
      "\n",
      "{0: 0.8097217435650479, 1: 0.5625782960997441, 2: 0.8348468846348633}\n",
      "agent2 died at time 99\n",
      "{0: 0.7972980406175312, 1: -3, 2: 0.0}\n",
      "agent0 died at time 120\n",
      "agent1 died at time 141\n",
      "Episode 47 end\n",
      "22107\n",
      "Losses {2: tensor(129.6550), 0: tensor(18.8856), 1: tensor(12.4067)}\n",
      "\n",
      "{0: 0.6870545307397147, 1: 0.8689927538456582, 2: 0.4928570272399808}\n",
      "{0: 0.6756400602201542, 1: 0.5907301053614522, 2: 0.6031126598974685}\n",
      "{0: 0.0, 1: 0.5414167162636194, 2: 0.547782098218254}\n",
      "agent1 died at time 162\n",
      "{0: -3, 2: -3}\n",
      "agent0 died at time 225\n",
      "agent2 died at time 225\n",
      "Episode 48 end\n",
      "22722\n",
      "Losses {1: tensor(125.9832), 0: tensor(404.9429), 2: tensor(48.0076)}\n",
      "\n",
      "{0: 0.4785680173572935, 1: 0.7250964633766337, 2: 0.5109372821820027}\n",
      "{0: -3, 1: 0.4819823056066893, 2: 0.7831902516856892}\n",
      "agent0 died at time 141\n",
      "{1: 0.7879417111569205, 2: 0.7960305201511677}\n",
      "{1: 0.38806313330770326, 2: 0.5978238646763362}\n",
      "agent1 died at time 204\n",
      "agent2 died at time 246\n",
      "Episode 49 end\n",
      "23316\n",
      "Losses {0: tensor(107.8983), 1: tensor(138.1372), 2: tensor(198.0539)}\n",
      "\n",
      "{0: 0.875627281988644, 1: 0.5757704046271575, 2: 0.7879930900089638}\n",
      "agent0 died at time 99\n",
      "{0: 0.6468742493865554, 1: 0.6401302771968007, 2: 0.592368613713923}\n",
      "agent2 died at time 120\n",
      "agent1 died at time 141\n",
      "Episode 50 end\n",
      "23679\n",
      "Losses {0: tensor(45.5708), 2: tensor(19.9677), 1: tensor(20.9781)}\n",
      "\n",
      "{0: 0.6768604031865899, 1: 0.575732569024062, 2: 0.7621896422761593}\n",
      "{0: 0.641659970211099, 1: 0.6193351656307254, 2: 0.5560031784148624}\n",
      "agent0 died at time 120\n",
      "{1: 0.39849108474718187, 2: 0.6391460093074144}\n",
      "agent1 died at time 162\n",
      "{2: 0.7984960411153654}\n",
      "agent2 died at time 225\n",
      "Episode 51 end\n",
      "24189\n",
      "Losses {0: tensor(49.2432), 1: tensor(34.5169), 2: tensor(1522.8380)}\n",
      "\n",
      "{0: 0.7876008925317057, 1: 0.725370254586869, 2: 0.5837977017566238}\n",
      "{0: 0.6284328570549433, 1: 0.8882866091988555, 2: 0.5138535566853526}\n",
      "agent2 died at time 141\n",
      "{0: 0.6558022736787448, 1: 0.5101116063108435}\n",
      "agent0 died at time 162\n",
      "agent1 died at time 183\n",
      "Episode 52 end\n",
      "24678\n",
      "Losses {2: tensor(12.4779), 0: tensor(9.8807), 1: tensor(227.4102)}\n",
      "\n",
      "{0: 0.4561793154187145, 1: 0.71557141417747, 2: 0.32026816892031373}\n",
      "agent1 died at time 99\n",
      "{0: 0.26165345760143166, 1: 0.5649283698711112, 2: 0.5792668638338376}\n",
      "agent0 died at time 141\n",
      "agent2 died at time 141\n",
      "Episode 53 end\n",
      "25062\n",
      "Losses {1: tensor(22.5341), 0: tensor(106.1658), 2: tensor(11.9876)}\n",
      "\n",
      "{0: 0.7971232168758204, 1: 0.7329136173643902, 2: 0.5660638419612234}\n",
      "{0: 0.5496408193029192, 1: 55, 2: 0.7155795716655943}\n",
      "agent0 died at time 141\n",
      "{1: 0.5417398532590625, 2: 0.8737808073301079}\n",
      "agent1 died at time 183\n",
      "agent2 died at time 183\n",
      "Episode 54 end\n",
      "25572\n",
      "Losses {0: tensor(113.5146), 1: tensor(9.0718), 2: tensor(11.0219)}\n",
      "\n",
      "{0: 0.726488697318334, 1: 0.6367461205374984, 2: 0.8654771460724551}\n",
      "{0: 0.7202782259820211, 1: 0.6748909793742386, 2: 0.505783211651903}\n",
      "agent0 died at time 141\n",
      "agent2 died at time 141\n",
      "{1: 0.25114163029951286}\n",
      "agent1 died at time 162\n",
      "Episode 55 end\n",
      "26019\n",
      "Losses {0: tensor(123.3190), 2: tensor(21.9006), 1: tensor(199.5225)}\n",
      "\n",
      "{0: 0.5504860184541029, 1: 0.9204210977338503, 2: 0.8848088968103709}\n",
      "{0: 0.6593832149563112, 1: 0.7924625525268353, 2: 0.562697191668505}\n",
      "agent2 died at time 141\n",
      "{0: 0.5676222692571606, 1: 0.26318246236537157}\n",
      "agent0 died at time 162\n",
      "agent1 died at time 183\n",
      "Episode 56 end\n",
      "26508\n",
      "Losses {2: tensor(102.1926), 0: tensor(167.3935), 1: tensor(87.5714)}\n",
      "\n",
      "{0: 0.7563158886087292, 1: 0.5730562202735336, 2: 0.7761581445042088}\n",
      "agent0 died at time 99\n",
      "{0: -3, 1: 0.329050975590957, 2: 0.5844806943293102}\n",
      "{1: -3, 2: -3}\n",
      "agent2 died at time 162\n",
      "{1: 0.23812817507415207}\n",
      "agent1 died at time 246\n",
      "Episode 57 end\n",
      "27018\n",
      "Losses {0: tensor(116.9852), 2: tensor(103.7310), 1: tensor(10.7736)}\n",
      "\n",
      "{0: 0.6545050493767657, 1: 0.769666941752268, 2: 0.7181774022619567}\n",
      "{0: 0.6508096759982755, 1: 0.5560917604221014, 2: 0.7193936275362383}\n",
      "agent2 died at time 120\n",
      "{0: 0.4848735401732719, 1: 0.6002332571607675}\n",
      "agent1 died at time 162\n",
      "{0: 0.8490411674796523}\n",
      "agent0 died at time 225\n",
      "Episode 58 end\n",
      "27528\n",
      "Losses {2: tensor(25.6024), 1: tensor(54.4373), 0: tensor(222.1218)}\n",
      "\n",
      "{0: 0.7116167012153302, 1: 0.8409673489368958, 2: 0.4447354337379281}\n",
      "{0: 0.33196482565324714, 1: 0.0003983120313945099, 2: 0.4697354232841051}\n",
      "{0: 0.3802931987484073, 1: 0.3662246074514368, 2: 0.5398483257957181}\n",
      "agent0 died at time 162\n",
      "agent2 died at time 162\n",
      "agent1 died at time 183\n",
      "Episode 59 end\n",
      "28038\n",
      "Losses {0: tensor(35.7291), 2: tensor(78.1847), 1: tensor(15.6634)}\n",
      "\n",
      "{0: 0.450709551503534, 1: 0.4636901287802331, 2: 0.6625107470678919}\n",
      "agent1 died at time 99\n",
      "{0: 0.5994414813643825, 1: 0.641305581875477, 2: 0.6224356463738963}\n",
      "agent2 died at time 120\n",
      "{0: 0.5978036465977841}\n",
      "{0: 0.6630050849107476}\n",
      "{0: 0.0}\n",
      "agent0 died at time 267\n",
      "Episode 60 end\n",
      "28527\n",
      "Losses {1: tensor(130.2143), 2: tensor(224.8443), 0: tensor(71.5378)}\n",
      "\n",
      "{0: 0.7292609477837413, 1: 0.5199106699889409, 2: 0.39089011862413414}\n",
      "{0: 0.6494763652105087, 1: 0.8747942821276308, 2: 0.46439052912857537}\n",
      "agent0 died at time 120\n",
      "agent2 died at time 141\n",
      "{1: 0.8078350008675881}\n",
      "agent1 died at time 183\n",
      "Episode 61 end\n",
      "28974\n",
      "Losses {0: tensor(45.7106), 2: tensor(18.9455), 1: tensor(15.9983)}\n",
      "\n",
      "{0: -3, 1: 0.7930509222251202, 2: 0.6265400139957427}\n",
      "agent0 died at time 99\n",
      "{0: 0.4049053942770465, 1: -3, 2: -3}\n",
      "{1: -3, 2: -3}\n",
      "agent1 died at time 162\n",
      "agent2 died at time 162\n",
      "Episode 62 end\n",
      "29400\n",
      "Losses {0: tensor(283.7528), 1: tensor(363.8452), 2: tensor(26.1753)}\n",
      "\n",
      "{0: 0.9180386626247741, 1: 0.5335365288179645, 2: 0.7277467660501453}\n",
      "{0: 0.6093161136531833, 1: 0.6504580563930492, 2: 0.60080096578843}\n",
      "{0: -3, 1: 0.22817441493570756, 2: 0.7719298568037198}\n",
      "agent0 died at time 162\n",
      "agent1 died at time 162\n",
      "{2: 0.7448243809969236}\n",
      "agent2 died at time 204\n",
      "Episode 63 end\n",
      "29931\n",
      "Losses {0: tensor(393.2986), 1: tensor(164.4037), 2: tensor(66.0367)}\n",
      "\n",
      "{0: 0.5309332982249064, 1: 0.4857926769669161, 2: 0.6709533511962941}\n",
      "{0: 0.6133481216914961, 1: 0.9330570047695287, 2: 0.6872399219680736}\n",
      "agent0 died at time 120\n",
      "agent2 died at time 120\n",
      "{1: -3}\n",
      "agent1 died at time 162\n",
      "Episode 64 end\n",
      "30336\n",
      "Losses {0: tensor(672.9952), 2: tensor(21.1465), 1: tensor(161.5199)}\n",
      "\n",
      "{0: 0.8742620091293368, 1: 0.4999943333179593, 2: 0.23291729846259468}\n",
      "agent1 died at time 99\n",
      "{0: 0.48733555943770845, 1: 0.6005865597151334, 2: 0.07608498880211889}\n",
      "agent0 died at time 120\n",
      "agent2 died at time 141\n",
      "Episode 65 end\n",
      "30699\n",
      "Losses {1: tensor(177.4426), 0: tensor(175.7156), 2: tensor(384.1074)}\n",
      "\n",
      "{0: 0.5368254432590687, 1: 0.6161535323885767, 2: 0.6917254648784414}\n",
      "{0: 0.7863173941535674, 1: -3, 2: 0.7315285885880138}\n",
      "agent0 died at time 141\n",
      "agent2 died at time 141\n",
      "{1: 0.6016958156144168}\n",
      "agent1 died at time 162\n",
      "Episode 66 end\n",
      "31146\n",
      "Losses {0: tensor(106.9850), 2: tensor(196.5784), 1: tensor(123.7047)}\n",
      "\n",
      "{0: 0.509480915755132, 1: 0.44684706251401096, 2: 0.39711816149276336}\n",
      "{0: 0.6761684322517436, 1: 0.7459084374676823, 2: 0.6037344258885895}\n",
      "agent0 died at time 120\n",
      "agent2 died at time 120\n",
      "agent1 died at time 141\n",
      "Episode 67 end\n",
      "31530\n",
      "Losses {0: tensor(526.2642), 2: tensor(149.8362), 1: tensor(122.6014)}\n",
      "\n",
      "{0: 0.7431383128766254, 1: 0.8043556323764998, 2: 55}\n",
      "{0: 0.582495661146996, 1: 0.6942159818996776, 2: 0.5604018788689968}\n",
      "agent0 died at time 120\n",
      "agent2 died at time 141\n",
      "{1: 0.6919720644064287}\n",
      "agent1 died at time 162\n",
      "Episode 68 end\n",
      "31956\n",
      "Losses {0: tensor(138.3035), 2: tensor(119.3033), 1: tensor(388.1917)}\n",
      "\n",
      "{0: 0.6977129873399235, 1: 0.5307483999919225, 2: 0.9378066944712068}\n",
      "agent0 died at time 99\n",
      "{0: 0.4751310079304678, 1: 0.7758198856525328, 2: 0.5269296829078985}\n",
      "agent1 died at time 141\n",
      "{2: 0.5748869014613118}\n",
      "agent2 died at time 162\n",
      "Episode 69 end\n",
      "32361\n",
      "Losses {0: tensor(64.7668), 1: tensor(147.7327), 2: tensor(508.3042)}\n",
      "\n",
      "{0: 0.5573300617974082, 1: 0.5983138347676733, 2: 0.0}\n",
      "{0: 0.8073404259132819, 1: 0.5613808912071254, 2: 0.7657611918943374}\n",
      "agent0 died at time 141\n",
      "{1: 0.831743727783935, 2: 0.3424285166002289}\n",
      "agent2 died at time 162\n",
      "{1: 0.0}\n",
      "agent1 died at time 204\n",
      "Episode 70 end\n",
      "32871\n",
      "Losses {0: tensor(110.9606), 2: tensor(61.3542), 1: tensor(1020.1641)}\n",
      "\n",
      "{0: -3, 1: 0.34819674996701444, 2: 0.7991911398683045}\n",
      "agent0 died at time 99\n",
      "{0: 0.32765031500637287, 1: 55, 2: -3}\n",
      "agent2 died at time 141\n",
      "{1: 0.7849903142340331}\n",
      "{1: 0.703283281471729}\n",
      "{1: -3}\n",
      "agent1 died at time 267\n",
      "Episode 71 end\n",
      "33381\n",
      "Losses {0: tensor(321.1314), 2: tensor(90.7485), 1: tensor(57.8027)}\n",
      "\n",
      "{0: 0.6335804891350858, 1: 0.12849530002547382, 2: 0.714194396332342}\n",
      "{0: -3, 1: -3, 2: -3}\n",
      "agent0 died at time 120\n",
      "agent2 died at time 141\n",
      "{1: 0.2481255909014417}\n",
      "agent1 died at time 162\n",
      "Episode 72 end\n",
      "33807\n",
      "Losses {0: tensor(174.8747), 2: tensor(261.9727), 1: tensor(73.3076)}\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m, obs \u001b[38;5;129;01min\u001b[39;00m next_obs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     39\u001b[0m     models[\u001b[38;5;28mid\u001b[39m]\u001b[38;5;241m.\u001b[39mpush_to_buffer(torch\u001b[38;5;241m.\u001b[39mtensor(curr_obs[\u001b[38;5;28mid\u001b[39m], dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat32),actions[\u001b[38;5;28mid\u001b[39m],rewards[\u001b[38;5;28mid\u001b[39m],torch\u001b[38;5;241m.\u001b[39mtensor(obs, dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat32),log_probs[\u001b[38;5;28mid\u001b[39m],terminations[\u001b[38;5;28mid\u001b[39m])\n\u001b[1;32m---> 40\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(terminations[\u001b[38;5;28mid\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;66;03m# loss = models[id].update_weights()\u001b[39;00m\n\u001b[0;32m     44\u001b[0m         losses[\u001b[38;5;28mid\u001b[39m] \u001b[38;5;241m=\u001b[39m loss\n",
      "File \u001b[1;32md:\\RL_Project\\Evolution-Simulation\\Models\\DDQN.py:157\u001b[0m, in \u001b[0;36mDoubleDQN.update_weights\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''This function updates the weights of the actor and the critic network based on the given state, action, reward and next_state\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;124;03mBatch:\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;124;03m    - states - torch.tensor: The state of the environment given as the input\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;124;03m    - non_final_mask - torch.tensor: tensor of the same size as the next_states which tells if the next state is terminal or not\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m--> 157\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# when buffer is not filled enough\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\RL_Project\\Evolution-Simulation\\Models\\ReplayBuffer\\ReplayBuffer.py:52\u001b[0m, in \u001b[0;36mReplayBuffer.sample\u001b[1;34m(self, batch_size, experience)\u001b[0m\n\u001b[0;32m     49\u001b[0m     probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# then usng this probability, sample the indices from the transition memory\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m batch_indices \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransition_memory\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# create the batch using the indices\u001b[39;00m\n\u001b[0;32m     54\u001b[0m batch \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransition_memory[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m batch_indices]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "episodes = 400\n",
    "\n",
    "for episode in tqdm(range(episodes)): #episodes loop\"\n",
    "\n",
    "    curr_obs,infos = env.reset() #reset environment after each episode\\n\",\n",
    "    # print(curr_obs[0][1])\n",
    "\n",
    "    losses = {}\n",
    "    while(env.agents): #until there are any surviving agents \n",
    "\n",
    "        actions = {}\n",
    "        log_probs = {}\n",
    "\n",
    "        for a in env.agents:\n",
    "            # print(curr_obs[a])\n",
    "            act, log_prob = models[a].select_action(state = torch.tensor(curr_obs[a], dtype = torch.float32))\n",
    "            actions[a] = act\n",
    "            log_probs[a] = log_prob\n",
    "\n",
    "        # for i in range(env.n_agents): #objects contains all of the models, get the corresponding actions from each policy\\n\",\n",
    "            \n",
    "        #     if i in env.agents:\n",
    "        #         act, log_prob = env.agents_objects[i].brain1.select_action(state = torch.tensor(curr_obs[i], dtype = torch.float32))\n",
    "        #         actions[i] = act\n",
    "        #         log_probs[i] = log_prob\n",
    "        #     else:\n",
    "        #         actions[i] = env.num_actions\n",
    "        #         log_probs[i] = 69\n",
    "            \n",
    "        # print(actions)\n",
    "\n",
    "        next_obs, rewards, terminations, truncations, infos = env.step(actions)\n",
    "        if env.timestep % 50 == 0:\n",
    "            # plot_grad_flow(models[0].policy_net.parameters())\n",
    "            print(rewards)\n",
    "        \n",
    "\n",
    "        for id, obs in next_obs.items():\n",
    "            models[id].push_to_buffer(torch.tensor(curr_obs[id], dtype = torch.float32),actions[id],rewards[id],torch.tensor(obs, dtype = torch.float32),log_probs[id],terminations[id])\n",
    "            loss = models[id].update_weights()\n",
    "\n",
    "            if(terminations[id] == True):\n",
    "                # loss = models[id].update_weights()\n",
    "                losses[id] = loss\n",
    "        \n",
    "        # for i in range(env.n_agents):\n",
    "\n",
    "        #     if i in env.agents:\n",
    "        #         env.agents_objects[i].brain1.push_to_buffer(torch.tensor(curr_obs[i], dtype = torch.float32),actions[i],rewards[i],torch.tensor(next_obs[i], dtype = torch.float32),log_probs[i],terminations[i])\n",
    "        #         env.agents_objects[i].brain1.update_weights() \n",
    "            \n",
    "        #     if i in env.justdie:\n",
    "        #         env.justdie[i].brain1.push_to_buffer(torch.tensor(curr_obs[i], dtype = torch.float32),actions[i],rewards[i],torch.tensor(next_obs[i], dtype = torch.float32),log_probs[i],terminations[i])\n",
    "        #         loss = env.justdie[i].brain1.update_weights() \n",
    "        #         del env.justdie[i]\n",
    "        #         losses[i] = loss\n",
    "        \n",
    "        curr_obs = next_obs\n",
    "    \n",
    "    print(f\"Episode {episode} end\")\n",
    "    print(len(ddqn_1.replay_buffer))\n",
    "    print(f\"Losses {losses}\")\n",
    "    print()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[0].steps_done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
